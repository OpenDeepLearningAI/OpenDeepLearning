# Deep dive into LLM

Embark on an in-depth exploration of LLMs with our comprehensive resources. This deep dive offers a thorough understanding of LLMs, equipping you with advanced knowledge and practical insights in the field.

<br />

import Image from "next/image";

<div style={{ position: "relative" }}>
  <Image
    src="/static/img/memes/llmsMeme.png"
    alt="LLMs meme"
    height="300"
    width="1000"
    priority
  />
</div>

### Courses

- [LLMs: Foundation Models from the Ground Up](https://www.edx.org/learn/computer-science/databricks-large-language-models-foundation-models-from-the-ground-up) by Databricks, provides an in-depth exploration of foundational models in LLMs, highlighting key innovations that fueled the rise of transformer-based models like <a class="link" target="_blank" href="https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/bert">BERT</a>, <a class="link" target="_blank" href="https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/openai-gpt">GPT</a>, and <a class="link" target="_blank" href="https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/t5">T5</a>. It also covers advanced techniques, such as Flash Attention, <a class="link" target="_blank" href="https://huggingface.co/docs/peft/conceptual_guides/lora">LoRa</a>, and <a class="link" target="_blank" href="https://huggingface.co/docs/peft/index">PEFT</a>, contributing to the ongoing enhancements of LLM capabilities, including applications like ChatGPT.

- [Mlabonne's LLM Course](https://github.com/mlabonne/llm-course) is a comprehensive guide to LLMs, featuring a roadmap, notebooks, and articles covering various aspects. The course includes topics like LLM training, inference optimization techniques, building frameworks, and more. It offers a step-by-step guide for entering the world of large language models. <a class="link" target="_blank" href="https://mlabonne.github.io/blog/">(website)</a>

- [Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) by Andrej Karpathy, where you'll embark on a journey to build neural networks from the ground up, all through code. Starting with the fundamentals of <a class="link" target="_blank" href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>, progress to crafting cutting-edge deep neural networks like <a class="link" target="_blank" href="https://huggingface.co/docs/transformers/model_doc/openai-gpt">GPT</a>. Language models serve as an excellent entry point into deep learning, with skills transferable across domains, making them our primary focus. <a class="link" target="_blank" href="https://karpathy.ai/zero-to-hero.html">(website)</a>

### Explainers

- [How Neural Networks Learned to Talk: A 30 Year History](https://www.youtube.com/watch?v=OFS90-FX6pg): This video explores the evolution of language models, from modest beginnings to the development of OpenAI's GPT models and hints at Q*. The journey delves into key moments in neural network research focused on next-word prediction, highlighting early experiments with small language models in the 1980s and significant contributions.

- [LLMs Process Explained what makes them tick & how they work](https://www.youtube.com/watch?v=_Pt-rGE4zEE&t=5s): By AemonAlgiz, covers foundational concepts like <a class="link" target="_blank" href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>, <a class="link" target="_blank" href="https://huggingface.co/docs/transformers/main_classes/tokenizer">tokenization</a>, <a class="link" target="_blank" href="https://huggingface.co/blog/getting-started-with-embeddings">embedding</a>, and <a class="link" target="_blank" href="https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6">positional encoding</a>. Dive into the magic of attention and multi-attention heads for enhanced comprehension, all presented with accessible clarity and depth, suitable for AI enthusiasts at all levels. It also addresses challenges and ongoing efforts in the field, such as accent adaptation and enhancing ASR system quality. If you're looking to delve into the details, then contemplate <a class="link" target="_blank" href="https://docs.google.com/presentation/d/1hQUd3pF8_2Gr2Obc89LKjmHL0DlH-uof9M0yFVd3FA4/mobilepresent?slide=id.g16197112905_0_0">Jason Wei's docs</a> on LLMs.

- [LLM Visualization](https://bbycroft.net/llm): Experience a 3D visualization and walkthrough of the LLM algorithm powering OpenAI's ChatGPT. Dive into the intricacies, exploring each addition and multiplication, and witness the entire process in action.

- [Prompt injection: What’s the worst that can happen?](https://simonwillison.net/2023/Apr/14/worst-that-can-happen/): Prompt injection represents a significant security concern within LLM applications, and while there is no flawless remedy, Simon Willison provides a comprehensive explanation of this issue in his post. Simon consistently produces exceptional content on AI-related topics.

- [LLM Visualization](https://bbycroft.net/llm): By [@BrendanBycroft](https://twitter.com/BrendanBycroft), an interactive 3D walkthrough of all the operations required in a single token inference for the `nano-gpt` architecture.

### Articles

- [ChatGPT Explained: A Normie's Guide To How It Works](https://www.jonstokes.com/p/chatgpt-explained-a-guide-for-normies) by Jon Stokes, An overview of ChatGPT, focusing on core concepts. Topics include token window, training data, rules, and interactive token usage for improved conversation-like interactions. It clarifies its structure without anthropomorphism.

- [The Scaling Hypothesis](https://gwern.net/scaling-hypothesis): Explore the Scaling Hypothesis, a captivating theory that posits larger AI models outperform smaller ones with ample data and resources. Delve into its impact on language models like GPT-3, controversies, applications, and ongoing debates among researchers. Discover the potential for achieving human-level or superhuman AI, and how organizations like EleutherAI are actively testing its limits through open-source models.

- [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html): Chip Huyen explores several significant hurdles encountered in developing LLM applications, offers solutions for tackling them, and highlights the most suitable use cases for these applications.

- [Chinchilla's wild implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications): This post delves into language model scaling laws, particularly those from the DeepMind paper introducing Chinchilla. Chinchilla, with 33-B parameters, defies the Scaling Hypothesis, highlighting the multifaceted role of factors like model architecture and data curation in performance.

- [GPT-4](https://openai.com/research/gpt-4): OpenAI's latest milestone, is a versatile multimodal model accepting text and image inputs, excelling in creative and technical writing. It generates, edits, and collaborates with users. It handles over 25k words, making it suitable for long-form content, conversations, and document analysis. Although advanced, it may have occasional reasoning errors and gullibility.

- [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) by Stephen Wolfram, traces the development of AI from simple neural networks to complex language models like ChatGPT that leverage massive datasets and computing power to produce remarkably natural conversational text, giving insight into the inner workings and capabilities of modern AI. 

- [The Waluigi Effect](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post): Delves into the Waluigi Effect and unusual "semiotic" occurrences in large language models like GPT-3/3.5/4 and their variants (ChatGPT, Sydney), providing mechanistic insights.

- [New models and developer products announced at DevDay](https://openai.com/blog/new-models-and-developer-products-announced-at-devday): OpenAI launches GPT-4 Turbo (128K context, lower prices), Assistants API for agent-like experiences, DALL-E 3 API, Whisper v3 ASR model, user-friendly GPT customization, Custom Models program, and reduced platform prices.

### State of LLMs

- [Intro to LLMs](https://www.youtube.com/watch?v=zjkBMFhNj_g) by Andrej Karpathy, provides a general-audience introduction to Large Language Models, the key technical element in systems like ChatGPT, Claude, and Bard. It covers their nature, future directions, analogies to current operating systems, and touches on security challenges in this emerging computing paradigm.

- [State of GPT](https://www.youtube.com/watch?v=bZQun8Y4L2A&t=86s) by Andrej Karpathy, Learn about the training pipeline of GPT assistants like ChatGPT, from <a class="link" target="_blank" href="https://huggingface.co/docs/tokenizers/index">tokenization</a> to <a class="link" target="_blank" href="https://huggingface.co/learn/nlp-course/chapter7/6"></a> pretraining, <a class="link" target="_blank" href="https://huggingface.co/docs/trl/main/en/sft_trainer">supervised finetuning</a>, and Reinforcement Learning from Human Feedback <a class="link" target="_blank" href="https://huggingface.co/blog/rlhf">(RLHF)</a>. Dive deeper into practical techniques and mental models for the effective use of these models, including prompting strategies, finetuning, the rapidly growing ecosystem of tools, and their future extensions.

### LLM Benchmarks

- [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard): An innovative benchmark platform designed for LLMs. <a class="link" target="_blank" href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo rating system</a> based leaderboard, inspired by competitive games like chess, encourage the entire community to participate by submitting new models, evaluating their performance, and engaging in the exciting world of LLM battles. <a class="link" target="_blank" href="https://arxiv.org/abs/2306.05685">(paper)</a> <a class="link" target="_blank" href="https://lmsys.org/blog/2023-05-03-arena/">(website)</a> 

- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard): A ranking by Hugging Face, comparing open source LLMs across a collection of standard benchmarks and tasks. It enables transparent comparisons on metrics like accuracy and compute efficiency across models to help guide appropriate model selection and usage for various applications.

- [Stanford HELM Leaderboard](https://crfm.stanford.edu/helm/v0.4.0/#/leaderboard): HELM is a dynamic language model benchmark, providing comprehensive coverage and addressing historical gaps in AI evaluations. It benchmarks models rigorously under standardized conditions, using a top-down approach to facilitate systematic scenario and metric selection.

- [Multi-task Language Understanding on MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu): A comparison between LLMs on the Multi-task Language Understanding <a class="link" target="_blank" href="https://paperswithcode.com/dataset/mmlu">(MMLU) dataset</a>. Assessing performance across 57 diverse tasks, from math to law, MMLU measures measuring multitask accuracy, identifying shortcomings, and tracking progress in language understanding. <a class="link" target="_blank" href="https://arxiv.org/abs/2009.03300">(paper)</a> <a class="link" target="_blank" href="https://github.com/hendrycks/test">(code)</a> 

### Explorations

- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT): An open-source demonstration of GPT-4's capabilities. It benchmarks agent performance, offering internet access, memory management, and text generation. It can complete tasks with minimal human intervention and self-prompt for various requests. <a class="link" target="_blank" href="https://news.agpt.co/">(website)</a> 

- [BabyAGI](https://github.com/yoheinakajima/babyagi) is an open-source Python library designed for training and assessing AGI agents in the BabyAI environment. This environment presents simple text-based games where agents must learn navigation and interaction to achieve goals. BabyAGI provides a user-friendly framework for working with the BabyAI environment and offers a collection of pre-trained models to kickstart training.

- [MemGPT](https://memgpt.ai/) expands context in LLMs by managing memory tiers and using interrupts for user interaction. It can analyze large documents, enabling conversational agents to evolve during long-term interactions. It's an OS-inspired system for extended context within LLMs. <a class="link" target="_blank" href="https://arxiv.org/abs/2310.08560">(paper)</a> 

- [Ollama](https://github.com/jmorganca/ollama) is a versatile software tool for running LLMs like Llama 2 on your local computer. It streamlines setup, optimizes GPU utilization, and consolidates model components into a single package, allowing for easy customization and model creation. <a class="link" target="_blank" href="https://ollama.ai/">(website)</a> 

- [Open Interpreter](https://openinterpreter.com/) enables LLMs to execute code on a user's computer for various tasks. It offers a natural-language interface for tasks like photo and video editing, PDF creation, Chrome browser control, and data analysis. Users interact with Open Interpreter through a ChatGPT-style terminal interface, allowing local execution of Python, JavaScript, Shell, and more.

- [AutoGen](https://microsoft.github.io/autogen/) is a versatile framework for LLM applications, facilitating the creation of conversational agents capable of collaborating on tasks. These customizable agents support human interaction, operate in diverse modes utilizing LLMs, human inputs, and tools.

- [GPT-FAST](https://github.com/pytorch-labs/gpt-fast): A PyTorch-native transformer text generation model by PyTorch Labs, is a lightweight and efficient tool for text generation and evaluation. Optimized for on-device LLM inference and 4-bit quantization performance on various hardware.

### Advancements

- [ReAct: Synergizing Reasoning and Acting in Language Models](https://blog.research.google/2022/11/react-synergizing-reasoning-and-acting.html): A novel paradigm fuses reasoning and acting in language models. It excels in language reasoning tasks, producing verbal traces and text actions simultaneously, enhancing dynamic reasoning, and adapting to external input for improved performance. <a class="link" target="_blank" href="https://arxiv.org/abs/2210.03629">(paper)</a> 

- [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models](https://meta-math.github.io/): MetaMath is a project for enhancing mathematical questions for language models. It builds the <a class="link" target="_blank" href="https://huggingface.co/datasets/meta-math/MetaMathQA">MetaMathQA dataset</a> and fine-tunes LLaMA-2 models, creating specialized mathematical reasoning models. Results show MetaMath's significant performance lead on GSM8K and MATH benchmarks, even surpassing models of the same size. <a class="link" target="_blank" href="https://huggingface.co/meta-math/MetaMath-7B-V1.0">(model)</a> <a class="link" target="_blank" href="https://github.com/meta-math/MetaMath">(code)</a> <a class="link" target="_blank" href="https://arxiv.org/abs/2309.12284">(paper)</a>

- [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html) (2023): Alpaca 7B, a model fine-tuned based on the LLaMA 7B model using 52k instruction-following demonstrations, exhibits qualitative similarity to OpenAI's text-davinci-003 in single-turn instruction following during our initial assessment. Remarkably, Alpaca maintains a compact size and is straightforward and cost-effective to replicate. <a class="link" target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca">(code)</a> <a class="link" target="_blank" href="https://arxiv.org/pdf/2303.16199.pdf">(paper)</a>

### Insights

- [State of AI Report 2023](https://www.stateof.ai/): Provides an exhaustive overview of AI, encompassing technology breakthroughs, industry developments, politics, safety, economic impacts, and future predictions. It encourages contributions from the AI community, fostering informed discussions about AI's future.

- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) (2023): Offers a comprehensive overview of the evolving landscape of LLMs, exploring their capabilities, applications, and challenges in NLP.

- [2023 State of AI in 14 Charts](https://hai.stanford.edu/news/2023-state-ai-14-charts): A snapshot of what happened this past year in AI research, education, policy, hiring, and more.

- [The AI Index Report: Measuring trends in AI](https://aiindex.stanford.edu/report/): By Stanford's HAI, compiling unbiased, globally sourced AI data. The 2023 report encompasses extra self-collected data and fresh analysis, focusing on foundation models, geopolitics, training costs, AI's environmental impact, and public opinion trends.

- [The state of AI in 2023: Generative AI’s breakout year](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-AIs-breakout-year) by McKinsey, discusses the explosive growth of generative AI tools.

### Papers

- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (2020): Scaling language models significantly improves task-agnostic, few-shot performance, competing with fine-tuning approaches. GPT-3, with 175 billion parameters, excels across NLP tasks without updates or fine-tuning, specified solely through text interaction. Strong performance is noted in translation, question-answering, and cloze tasks, with challenges and methodological issues identified in certain datasets.

- [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) (2023): Initial assessment conducted by Microsoft Research on GPT-4, the most sophisticated LLM currently available, in comparison to human cognitive abilities.
