# Web User Interface for LLMs

Creating a web user interface for LLM applications is a crucial stride in harnessing the power of language models. Whether you've fine-tuned a custom LLM from an open-source model or you intend to run an open-source model on your local machine or in the cloud, resources and tools are readily available to support you in deploying it within a web interface.

<br/>

import Image from "next/image";

<div style={{ position: "relative" }}>
  <Image
    src="/static/img/webUI.png"
    alt="Web UI img"
    height="300"
    width="850"
    priority
  />
</div>

These dedicated resources aim to simplify the process of integrating your language model into a user-friendly web environment, expanding its accessibility and enhancing its usefulness for a wider audience.

### Web UI for LLMs

- [Hugging Face Chat UI](https://github.com/huggingface/chat-ui): By Hugging Face, an open-source codebase, powers the HuggingChat app. It enables customizable chat interfaces for AI models, with features like Amazon SageMaker support, custom system prompts, and integration with transfer learning.

- [Gradio Guide](https://www.gradio.app/guides) is a Python library, enables fast creation and sharing of web interfaces for ML models, APIs, and data science workflows. Installation is simple with pip, and you create interfaces by defining functions. Sharing is easy, and Gradio offers layout control and web component embedding, making it a versatile tool for showcasing and collaborating on models.

- [Oobabooga Text Generation WebUI](https://github.com/oobabooga/text-generation-webui): By oobabooga is a GitHub repository, features a text generation web UI based on Gradio, released under the MIT License, supports various models, and offers different interface modes. Extensions enhance functionality, and documentation is available. The developer addresses reported issues promptly.

- [Text Generation WebUI Colab](https://github.com/camenduru/text-generation-webui-colab) is a GitHub repository, authored by camenduru, features a Colab notebook showcasing a Gradio web UI for LLMs. It's under the Apache License 2.0. The UI supports various models and interface modes.

- [Streamlit Docs](https://docs.streamlit.io/): Streamlit is an open-source Python library for creating and sharing custom ML web apps, user-friendly, enables fast development, seamlessly integrates with Python libraries, offers live updates, and has a cloud-based platform for deployment and sharing.

### Explainers

- [UX for Language User Interfaces (LLM Bootcamp)](https://www.youtube.com/watch?v=l5mG4z343qg): By The Full Stack, discuss user experience design for Language User Interfaces (LUIs) powered by LLMs. Topics include user-centered design principles, emerging UX patterns, case studies like GitHub Copilot and Bing Chat, and the role of pareidolia in LUIs.

- [Fine-Tune Language Models with LoRA! OobaBooga Walkthrough and Explanation](https://www.youtube.com/watch?v=7pdEK9ckDQ8): By AemonAlgiz, delves into LoRA (Low-Rank Approximation) for fine-tuning LLMs. It explains LoRA's role in reducing memory usage, demonstrates implementation with oobabooga's text generation web UI, and caters to all skill levels. Learn the linear algebra concepts and crucial hyperparameters for efficient language model fine-tuning.

- [PEFT LoRA Finetuning With Oobabooga!](https://www.youtube.com/watch?v=LjJ8hgnTYfs): Again by AemonAlgiz, fine-tuning LLMs using LoRAs with a focus on models like Alpaca and StableLM. The video covers key considerations, practical data transformations, preprocessing techniques, and real-time demonstrations using Oogabooga WebUI. This serves as a practical guide for those seeking to fine-tune their language models and sets the stage for the next video, which explores the [Hyena Hierarchy](https://arxiv.org/abs/2302.10866) paper's potential impact on LLMs.

### Guides

- [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index): The Hugging Face website offers a Text Generation Inference (TGI) guide for deploying LLMs. TGI optimizes popular LLMs, like Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and T5, with features like Tensor Parallelism, Token Streaming, and Continuous Batching. It's used by projects like Hugging Chat, OpenAssistant, and nat.dev, making it a valuable resource for LLM deployment research.
