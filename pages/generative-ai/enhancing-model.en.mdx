# Enhancing Model: Beyond Fine-Tuning

Enhancing models involves refining AI models using methods like LoRA, quantization, and RLHF. This process fine-tunes hyperparameters, increases training data, and optimizes architecture for improved performance, offering better accuracy and faster training. Enhanced models are crucial for various applications, from natural language processing to computer vision, offering more reliable and efficient outcomes.

<br/>

import Image from "next/image";

<div style={{ position: "relative" }}>
  <Image
    src="/static/img/memes/tuneMeme.png"
    alt="tuning model meme"
    height="300"
    width="850"
    priority
  />
</div>

### Explainers

- [What's wrong with LLMs and what we should be building instead](https://www.youtube.com/watch?v=cEyHsMzbZBs): By Tom Dietterich discusses the limitations of LLM and proposes alternative approaches for building more effective AI systems. The video provides insights into the challenges of LLMs and the potential solutions for creating better AI models.

- [LLaMa GPTQ 4-Bit Quantization. Billions of Parameters Made Smaller and Smarter. How Does it Work?](https://www.youtube.com/watch?v=mii-xFaPCrA): This video By AemonAlgiz, elucidates the 4-bit quantization method employed in LLaMa GPTQ models. It effectively reduces memory usage, enhances efficiency while preserving performance, delves into the quantization process, its influence on model capabilities, and the underlying mathematical principles.

- [Low-rank Adaption of LLMs: Explaining the Key Concepts Behind LoRA (part 1)](https://www.youtube.com/watch?v=dA-NhCtrrVE) & [part 2](https://www.youtube.com/watch?v=iYr1xZn26R8): This video by Chris Alexiuk, explains LoRA's importance for cost-effective Transformer fine-tuning. LoRA employs low-rank matrix decompositions to reduce training costs for LLM. It adapts low-rank factors instead of full weight matrices, offering memory and performance benefits. A second part covers implementing LoRA for fine-tuning on the SQuADv2 dataset.

- [Reinforcement Learning from Human Feedback: From Zero to chatGPT](https://www.youtube.com/watch?v=2MBJOuVq380) by Hugging Face discusses RLHF and its role in enhancing ML tools like ChatGPT. The talk provides an overview of interconnected ML models, NLP, and RL fundamentals to understand RLHF in LLMs, concluding with open questions in RLHF.

### Articles

- [List of Open Sourced Fine-Tuned LLM](https://sungkim11.medium.com/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76) by Sung Kim, this ongoing list catalogs open-sourced, fine-tuned LLMs you can run locally on your computer, grouped and sub-grouped by model type, it aims to serve as a useful reference for practitioners exploring leveraging powerful LLMs. By compiling fine-tuned models by their pretrained versions, the list helps users identify and compare the available options for leveraging in language projects.

- [Learning from Human Preferences](https://openai.com/research/learning-from-human-preferences) (2017) by OpenAI, delves into RLHF for AI model training. RLHF enables AI models to optimize behavior based on human preferences. The process involves collecting human feedback, building a reward model, and training the AI model through Reinforcement Learning. Challenges include reliance on human evaluators and potential for tricky policies. OpenAI is exploring various feedback types to improve training effectiveness.

- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf) by Hugging Face, RLHF integrates human data labels into RL optimization, emphasizing training helpful and safe AI models, especially in language tasks. Hugging Face advances RLHF with tools like the TRL library, optimizing for scalability. This approach enhances AI model performance, safety, reliability, interpretability, and alignment with human values.

- [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) by Sebastian Raschka, delves into the training of modern LLMs, outlines the canonical three-step training process, with a spotlight on RLHF and emerging alternatives like [The Wisdom of Hindsight](https://arxiv.org/abs/2302.05206) (2023) & [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) (2023) Ongoing research aims to enhance LLM performance and alignment with human preferences.

- [RLHF](https://huyenchip.com/2023/05/02/rlhf.html) by Chip Huyen, delves into RLHF for LLM training. RLHF employs a reward model to optimize LLMs for improved response quality. The process involves training the reward model and refining LLM responses, discusses RLHF's integration into LLM development phases and explores hypotheses about its effectiveness. Ongoing research aims to enhance LLM performance and safety through RLHF and alternative approaches.

- [Retrieval Augmented Generation: Streamlining the creation of intelligent NLP models](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) (2020): Retrieval Augmented Generation (RAG), developed by Meta AI, is an end-to-end differentiable model. Combining information retrieval with a seq2seq generator, it enhances NLP models by enabling access to up-to-date information, resulting in more specific, diverse, and factual language generation compared to state-of-the-art seq2seq models. [(paper)](https://arxiv.org/abs/2005.11401)

- [Improving language models by retrieving from trillions of tokens](https://www.deepmind.com/blog/improving-language-models-by-retrieving-from-trillions-of-tokens) (2021): By DeepMind, introduces RETRO (Retrieval Enhanced Transformers). Combining transformers with retrieval from a vast text database enhances models, offering improved specificity, diversity, factuality, and safety in text generation. Scaling the retrieval database to trillions of tokens benefits LLM. [(paper)](https://arxiv.org/abs/2112.04426)

- [H3: Language Modeling with State Space Models and (Almost) No Attention](https://hazyresearch.stanford.edu/blog/2023-01-20-h3) (2022): By Hazy Research (Stanford), introduces H3, a state space model combining GPT-Neo and GPT-2 strengths, offering superior perplexity scores with fewer parameters. H3 outperforms on various tasks, demonstrating its efficiency. Scaling and future research challenges are discussed. [(paper)](https://arxiv.org/abs/2212.14052)

- [Can Longer Sequences Help Take the Next Leap in AI?](https://ai.stanford.edu/blog/longer-sequences-next-leap-ai/) (2022): By Stanford AI lab, explores how extending sequence length benefits deep learning. Longer sequences can enhance AI in text processing and computer vision, boosting insight quality and opening new learning paradigms, such as in-context learning and story generation. Research in this area is exciting, with vast potential yet to be fully understood. [(paper)](https://arxiv.org/abs/2205.14135)

### Reference

- [GPT-3.5 Turbo fine-tuning](https://platform.openai.com/docs/guides/fine-tuning): OpenAI now offers fine-tuning for GPT-3.5 Turbo via an API, allowing developers to customize models for specific needs. Initial tests have demonstrated that fine-tuned GPT-3.5 Turbo can excel in certain tasks, rivalling base GPT-4 capabilities. [(blog)](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates)

- [Fine-tuning](https://platform.openai.com/docs/guides/fine-tuning): Learn how to customize a model for your application.

- [Pinecone learning center](https://www.pinecone.io/learn/): Numerous LLM applications adopt a vector search approach. Pinecone's educational hub, while labeled as vendor content, provides highly valuable guidance on constructing within this framework.

- [LangChain docs](https://python.langchain.com/docs/get_started/introduction): LangChain serves as the primary orchestration layer for LLM applications, seamlessly integrating with nearly every component in the stack. Consequently, their documentation serves as a valuable resource, offering comprehensive insights into the entire stack's composition and interactions.

- [Introducing GPTs](https://openai.com/blog/introducing-gpts): OpenAI introduces GPTs, customized ChatGPT variants, for specific needs. Users can easily enhance their utility without coding, suitable for personal, company, or public applications.

### Papers

- [Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf) (2017): Exploration of reinforcement learning in gaming and robotics domains revealed its remarkable utility for Language Models.

- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) (2022): Microsoft's study proposed an efficient method for training LLMs on new data, offering a community-standard approach, particularly for image models.

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) (2021): presents a technique to adapt large language models to specific tasks using low-rank matrix factorization. It enhances efficiency and performance, reducing parameters compared to other approaches. Experiments confirm its effectiveness.

- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) (2023): introduces an efficient finetuning technique. It reduces memory usage, enabling the finetuning of a 65B parameter model on a single 48GB GPU. QLoRA combines quantization and low-rank matrix factorization to shrink memory usage while maintaining competitive performance on various tasks. The paper also discusses the challenges of finetuning large language models and suggests potential avenues for future. 

- [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323) (2022): The paper unveils GPTQ, a novel weight quantization method, which effectively compresses GPT models. GPTQ outperforms existing methods in accuracy and compression, enhancing memory efficiency and inference speed for transformer-based models. 