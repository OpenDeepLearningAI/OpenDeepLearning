# Reinforcement Learning Papers

[Paper with Code RL section](https://paperswithcode.com/task/reinforcement-learning-1):  Provides access to research papers along with the corresponding code.

- [Never Give Up: Learning Directed Exploration Strategies](https://arxiv.org/abs/2002.06038) (2020)

## Key Papers

- [Q-learning](https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf) (1992): Introduces the Q-learning algorithm, one of the fundamental algorithms in RL.
- [Policy invariance under reward transformations: Theory and application to reward shaping](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf) (1999): Discusses the invariance of policies under reward transformations and the concept of reward shaping.
- [Learning to Predict by the Methods of Temporal Differences](https://www.researchgate.net/publication/225264698_Learning_to_Predict_by_the_Method_of_Temporal_Differences) (1988): Introduced the temporal difference (TD) learning algorithm, which is a model-free method for learning value functions in RL.
- [Actor-Critic Algorithms](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) (2003): Introduced the actor-critic architecture, which is a model-based method for learning policies in RL.

## Deep Reinforcement Learning

### Model-Free RL

#### Deep Q-Learning

- [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) (2013): Presents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.
- [Deep Recurrent Q-Learning for Partially Observable MDPs](https://arxiv.org/abs/1507.06527) (2015): Proposes a deep recurrent Q-learning algorithm for partially observable Markov decision processes.
- [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) (2015): Introduces a dueling network architecture for deep reinforcement learning that separates the estimation of state values and state-dependent action advantages.
- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) (2015): Proposes a double Q-learning algorithm for deep reinforcement learning that reduces overestimation of action values.
- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) (2015): Introduces a prioritized experience replay mechanism for deep reinforcement learning that improves sample efficiency and learning speed.
- [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298) (2017): Combines several improvements to deep reinforcement learning, including dueling networks, double Q-learning, and prioritized experience replay, to achieve state-of-the-art performance on Atari games.

#### Policy Gradients

- [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783) (2016): Proposes asynchronous methods for deep reinforcement learning that improve sample efficiency and learning speed.
- [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) (2015): Introduces a trust region optimization method for policy optimization in reinforcement learning that improves stability and sample efficiency.
- [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438) (2015): Proposes a generalized advantage estimation method for continuous control tasks in reinforcement learning that improves sample efficiency and learning speed.
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) (2017): Introduces a family of proximal policy optimization algorithms for reinforcement learning that improve sample efficiency and stability.
- [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/abs/1707.02286) (2017): Demonstrates the emergence of diverse locomotion behaviors in simulated environments using deep reinforcement learning.
- [Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation](https://arxiv.org/abs/1708.05144) (2017): Proposes a scalable trust-region method for deep reinforcement learning that uses Kronecker-factored approximation to improve sample efficiency and learning speed.
- [Sample Efficient Actor-Critic with Experience Replay](https://arxiv.org/abs/1611.01224) (2016): Introduces a sample-efficient actor-critic algorithm with experience replay for reinforcement learning that improves sample efficiency and learning speed.
- [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290) (2018): Proposes a soft actor-critic algorithm for deep reinforcement learning that maximizes entropy and improves exploration.

#### Deterministic Policy Gradients

- [Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf) (2014): Introduces a deterministic policy gradient algorithm for reinforcement learning that improves sample efficiency and stability.
- [Continuous Control With Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971) (2015): Demonstrates the effectiveness of deep reinforcement learning for continuous control tasks.
- [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) (2018): Addresses the problem of function approximation error in actor-critic methods for reinforcement learning.

#### Distributional RL

- [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887) (2017): Presents a distributional perspective on reinforcement learning that improves sample efficiency and learning speed.
- [Distributional Reinforcement Learning with Quantile Regression](https://arxiv.org/abs/1710.10044) (2017): Proposes a distributional reinforcement learning algorithm that uses quantile regression to estimate value distributions.
- [Implicit Quantile Networks for Distributional Reinforcement Learning](https://arxiv.org/abs/1806.06923) (2018): Introduces implicit quantile networks for distributional reinforcement learning that improve sample efficiency and learning speed.
- [Dopamine: A Research Framework for Deep Reinforcement Learning](https://openreview.net/forum?id=ByG_3s09KX) (2018) [(code)](https://github.com/google/dopamine): Provides a research framework for deep reinforcement learning that includes a suite of environments and baselines.

#### Policy Gradients with Action-Dependent Baselines

- [Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic](https://arxiv.org/abs/1611.02247) (2016): Proposes a sample-efficient policy gradient algorithm with an off-policy critic for reinforcement learning that improves sample efficiency and learning speed.
- [Action-depedent Control Variates for Policy Optimization via Stein’s Identity,](https://arxiv.org/abs/1710.11198) (2017): Proposes a control variate method for policy optimization in reinforcement learning that improves sample efficiency and stability.
- [The Mirage of Action-Dependent Baselines in Reinforcement Learning](https://arxiv.org/abs/1802.10031) (2018): Critiques the use of action-dependent baselines in reinforcement learning and proposes alternative methods.

#### Path-Consistency Learning

- [Bridging the Gap Between Value and Policy Based Reinforcement Learning](https://arxiv.org/abs/1702.08892) (2017): Proposes a method for bridging the gap between value-based and policy-based reinforcement learning.
- [Trust-PCL: An Off-Policy Trust Region Method for Continuous Control](https://arxiv.org/abs/1707.01891) (2017): Introduces an off-policy trust region method for continuous control in reinforcement learning that improves sample efficiency and stability.

#### Other Directions for Combining Policy-Learning & Q-Learning

- [Combining Policy Gradient and Q-learning](https://arxiv.org/abs/1611.01626) (2016): Combines policy gradient and Q-learning methods for reinforcement learning to improve sample efficiency and stability.
- [The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning](https://arxiv.org/abs/1704.04651) (2017): Introduces a fast and sample-efficient actor-critic algorithm for reinforcement learning that improves sample efficiency and learning speed.
- [Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning](http://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning) (2017): Proposes an interpolated policy gradient algorithm for deep reinforcement learning that combines on-policy and off-policy gradient estimation.
- [Equivalence Between Policy Gradients and Soft Q-Learning](https://arxiv.org/abs/1704.06440) (2017): Shows the equivalence between policy gradients and soft Q-learning in reinforcement learning.

#### Evolutionary Algorithms

- [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/abs/1703.03864) (2017): Explores the use of evolution strategies, a class of black box optimization algorithms, as an alternative to popular reinforcement learning techniques.

### Exploration

#### Intrinsic Motivation

- [VIME: Variational Information Maximizing Exploration](https://arxiv.org/abs/1605.09674) (2016): Proposes a variational information maximizing exploration method for reinforcement learning that improves exploration efficiency.
- [Unifying Count-Based Exploration and Intrinsic Motivation](https://arxiv.org/abs/1606.01868) (2016): Unifies count-based exploration and intrinsic motivation methods for reinforcement learning to improve exploration efficiency.
- [Count-Based Exploration with Neural Density Models](https://arxiv.org/abs/1703.01310) (2017): Proposes a count-based exploration method for reinforcement learning that uses neural density models to improve exploration efficiency.
- [#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning](https://arxiv.org/abs/1611.04717) (2016): Studies the effectiveness of count-based exploration methods for deep reinforcement learning.
- [EX2: Exploration with Exemplar Models for Deep Reinforcement Learning](https://arxiv.org/abs/1703.01260) (2017): Proposes an exploration method for deep reinforcement learning that uses exemplar models to improve exploration efficiency.
- [Curiosity-driven Exploration by Self-supervised Prediction](https://arxiv.org/abs/1705.05363) (2017): Proposes a curiosity-driven exploration method for reinforcement learning that uses self-supervised prediction to improve exploration efficiency.
- [Large-Scale Study of Curiosity-Driven Learning](https://arxiv.org/abs/1808.04355) (2018): Conducts a large-scale study of curiosity-driven learning in reinforcement learning.
- [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894) (2018): Proposes an exploration method for reinforcement learning that uses random network distillation to improve exploration efficiency.

#### Unsupervised RL

- [Variational Intrinsic Control](https://arxiv.org/abs/1611.07507) (2016): Proposes a variational intrinsic control method for reinforcement learning that improves exploration efficiency.
- [Diversity is All You Need: Learning Skills without a Reward Function](https://arxiv.org/abs/1802.06070) (2018): Proposes a method for learning skills without a reward function in reinforcement learning that improves sample efficiency.
- [Variational Option Discovery Algorithms](https://arxiv.org/abs/1807.10299) (2018): Proposes a variational option discovery algorithm for reinforcement learning that improves sample efficiency.

## Transfer and Multitask RL

- [Progressive Neural Networks](https://arxiv.org/abs/1606.04671) (2016): Proposes a progressive neural network architecture for reinforcement learning that improves sample efficiency.
- [Universal Value Function Approximators](http://proceedings.mlr.press/v37/schaul15.pdf) (2015): Proposes a universal value function approximator for reinforcement learning that improves sample efficiency.
- [The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously](https://arxiv.org/abs/1707.03300) (2017): Proposes a method for learning to solve multiple continuous control tasks simultaneously in reinforcement learning.
- [PathNet: Evolution Channels Gradient Descent in Super Neural Networks](https://arxiv.org/abs/1701.08734) (2017): Proposes a method for combining evolution and gradient descent in neural network training for reinforcement learning.
- [Mutual Alignment Transfer Learning](https://arxiv.org/abs/1707.07907) (2017): Proposes a mutual alignment transfer learning method for reinforcement learning that improves sample efficiency.
- [Learning an Embedding Space for Transferable Robot Skills](https://openreview.net/forum?id=rk07ZXZRb&noteId=rk07ZXZRb) (2018): Proposes a method for learning an embedding space for transferable robot skills in reinforcement learning.
- [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495) (2017): Proposes a hindsight experience replay method for reinforcement learning that improves sample efficiency.

## Hierarchy

- [Strategic Attentive Writer for Learning Macro-Actions](https://arxiv.org/abs/1606.04695) (2016): Proposes a strategic attentive writer method for learning macro-actions in reinforcement learning that improves sample efficiency.
- [FeUdal Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1703.01161) (2017): Proposes a feudal network architecture for hierarchical reinforcement learning that improves sample efficiency.
- [Data-Efficient Hierarchical Reinforcement Learning](https://arxiv.org/abs/1805.08296)
  (2018): Proposes a data-efficient hierarchical reinforcement learning method that improves sample efficiency.

## Memory

- [Model-Free Episodic Control](https://arxiv.org/abs/1606.04460) (2016): Proposes a model-free episodic control method for reinforcement learning that improves sample efficiency.
- [Neural Episodic Control](https://arxiv.org/abs/1703.01988) (2017): Proposes a neural episodic control method for reinforcement learning that improves sample efficiency.
- [Neural Map: Structured Memory for Deep Reinforcement Learning](https://arxiv.org/abs/1702.08360) (2017): Proposes a neural map architecture for reinforcement learning that uses structured memory to improve sample efficiency.
- [Unsupervised Predictive Memory in a Goal-Directed Agent](https://arxiv.org/abs/1803.10760) (2018): Proposes an unsupervised predictive memory method for goal-directed agents in reinforcement learning that improves sample efficiency.
- [ Relational Recurrent Neural Networks](https://arxiv.org/abs/1806.01822) (2018): Proposes a relational recurrent neural network architecture for reinforcement learning that improves sample efficiency.

## Model-Based RL

- [Imagination-Augmented Agents for Deep Reinforcement Learning](https://arxiv.org/abs/1707.06203) (2017): Proposes an imagination-augmented agent method for reinforcement learning that improves sample efficiency.
- [Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning](https://arxiv.org/abs/1708.02596) (2017): Proposes a neural network dynamics method for model-based deep reinforcement learning that improves sample efficiency.
- [Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning](https://arxiv.org/abs/1803.00101) (2018): Proposes a model-based value expansion method for model-free reinforcement learning that improves sample efficiency.
- [Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion](https://arxiv.org/abs/1807.01675) (2018): Proposes a sample-efficient reinforcement learning method with stochastic ensemble value expansion that improves sample efficiency.
- [Model-Ensemble Trust-Region Policy Optimization](https://openreview.net/forum?id=SJJinbWRZ&noteId=SJJinbWRZ) (2018): Proposes a model-ensemble trust-region policy optimization method for reinforcement learning that improves sample efficiency.
- [Model-Based Reinforcement Learning via Meta-Policy Optimization](https://arxiv.org/abs/1809.05214) (2018): Proposes a model-based reinforcement learning method via meta-policy optimization that improves sample efficiency.
- [Recurrent World Models Facilitate Policy Evolution](https://arxiv.org/abs/1809.01999) (2018): Proposes a recurrent world models method for policy evolution in reinforcement learning that improves sample efficiency.
- [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) (2017): Proposes a general reinforcement learning algorithm for mastering chess and shogi by self-play that achieves superhuman performance.
- [Thinking Fast and Slow with Deep Learning and Tree Search](https://arxiv.org/abs/1705.08439) (2017): Proposes a thinking fast and slow method for reinforcement learning that combines deep learning and tree search to improve sample efficiency.

## Meta-RL

- [RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning](https://arxiv.org/abs/1611.02779) (2016): Proposes an RL^2 method for fast reinforcement learning via slow reinforcement learning that improves sample efficiency.
- [Learning to Reinforcement Learn](https://arxiv.org/abs/1611.05763) (2016): Proposes a learning to reinforcement learn method for meta-reinforcement learning that improves sample efficiency.
- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400) (2017): Proposes a model-agnostic meta-learning method for fast adaptation of deep networks in reinforcement learning that improves sample efficiency.
- [A Simple Neural Attentive Meta-Learner](https://openreview.net/forum?id=B1DmUzWAW&noteId=B1DmUzWAW) (2018): Proposes a simple neural attentive meta-learner method for meta-reinforcement learning that improves sample efficiency.

## Scaling RL

- [Accelerated Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1803.02811) (2018): Proposes accelerated methods for deep reinforcement learning that improve sample efficiency and learning speed.
- [IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures](https://arxiv.org/abs/1802.01561) (2018): Proposes an importance weighted actor-learner architecture for scalable distributed deep reinforcement learning that improves sample efficiency and learning speed.
- [Distributed Prioritized Experience Replay](https://openreview.net/forum?id=H1Dy---0Z) (2018): Proposes a distributed prioritized experience replay method for deep reinforcement learning that improves sample efficiency and learning speed.
- [Recurrent Experience Replay in Distributed Reinforcement Learning](https://openreview.net/forum?id=r1lyTjAqYX) (2018): Proposes a recurrent experience replay method for distributed reinforcement learning that improves sample efficiency and learning speed.
- [RLlib: Abstractions for Distributed Reinforcement Learning](https://arxiv.org/abs/1712.09381) (2017): Proposes RLlib, a library of abstractions for distributed reinforcement learning that improves sample efficiency and learning speed. [(docs)](https://ray.readthedocs.io/en/latest/rllib.html)

## RL in the Real World

- [Benchmarking Reinforcement Learning Algorithms on Real-World Robots](https://arxiv.org/abs/1809.07731) (2018): Conducts a benchmarking study of reinforcement learning algorithms on real-world robots.
- [Learning Dexterous In-Hand Manipulation](https://arxiv.org/abs/1808.00177) (2018): Proposes a method for learning dexterous in-hand manipulation skills in reinforcement learning that improves sample efficiency.
- [QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation](https://arxiv.org/abs/1806.10293) (2018): Proposes a scalable deep reinforcement learning method for vision-based robotic manipulation that improves sample efficiency.
- [Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform](https://arxiv.org/abs/1811.00260) (2018): Introduces Horizon, Facebook's open-source applied reinforcement learning platform.

## Safety

- [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565) (2016): Discusses concrete problems in AI safety, including reinforcement learning.
- [Constrained Policy Optimization](https://arxiv.org/abs/1705.10528) (2017): Proposes a constrained policy optimization method for reinforcement learning that improves safety and stability.
- [Safe Exploration in Continuous Action Spaces](https://arxiv.org/abs/1801.08757) (2018): Proposes a safe exploration method for reinforcement learning in continuous action spaces that improves safety and stability.
- [Trial without Error: Towards Safe Reinforcement Learning via Human Intervention](https://arxiv.org/abs/1707.05173) (2017): Proposes a trial without error method for safe reinforcement learning via human intervention that improves safety and stability.
- [Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning](https://arxiv.org/abs/1711.06782) (2017): Proposes a learning to reset method for safe and autonomous reinforcement learning that improves safety and stability.

## Imitation Learning and Inverse Reinforcement Learning

- [Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy](http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf) (2010): Proposes a principle of maximum causal entropy for modeling purposeful adaptive behavior in reinforcement learning.
- [Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization](https://arxiv.org/abs/1603.00448) (2016): Proposes a guided cost learning method for deep inverse optimal control via policy optimization in reinforcement learning.
- [Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476) (2016): Proposes a generative adversarial imitation learning method for reinforcement learning that improves sample efficiency.
- [DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills](https://xbpeng.github.io/projects/DeepMimic/2018_TOG_DeepMimic.pdf) (2018): Proposes a deep mimic method for example-guided deep reinforcement learning of physics-based character skills that improves sample efficiency.
- [Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow](https://arxiv.org/abs/1810.00821) (2018): Proposes a variational discriminator bottleneck method for improving imitation learning, inverse RL, and GANs by constraining information flow in reinforcement learning.
- [One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL](https://arxiv.org/abs/1810.05017) (2018): Proposes a one-shot high-fidelity imitation method for training large-scale deep nets with reinforcement learning that improves sample efficiency.

## Reproducibility, Analysis, and Critique

- [Benchmarking Deep Reinforcement Learning for Continuous Control](https://arxiv.org/abs/1604.06778) (2016): Conducts a benchmarking study of deep reinforcement learning algorithms for continuous control.
- [Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control](https://arxiv.org/abs/1708.04133) (2017): Conducts a reproducibility study of benchmarked deep reinforcement learning tasks for continuous control.
- [Deep Reinforcement Learning that Matters](https://arxiv.org/abs/1709.06560) (2017): Discusses the importance of deep reinforcement learning research that addresses real-world problems.
- [Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods](https://arxiv.org/abs/1810.02525) (2018): Conducts an empirical analysis of gradient descent optimization in policy gradient methods for reinforcement learning.
- [Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?](https://arxiv.org/abs/1811.02553) (2018): Discusses the definition and properties of policy gradient algorithms in reinforcement learning.
- [Simple Random Search Provides a Competitive Approach to Reinforcement Learning](https://arxiv.org/abs/1803.07055) (2018): Proposes a simple random search method for reinforcement learning that achieves competitive performance.
- [Benchmarking Model-Based Reinforcement Learning](https://arxiv.org/abs/1907.02057) (2019): Proposes a benchmarking library for (MBRL) algorithms and environments to facilitate research and comparison of MBRL methods.

## Classic Papers in RL Theory or Review

- [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) (2000): Proposes a policy gradient method for reinforcement learning with function approximation that improves sample efficiency.
- [An Analysis of Temporal-Difference Learning with Function Approximation](http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf) (1997): Conducts an analysis of temporal-difference learning with function approximation in reinforcement learning.
- [Reinforcement Learning of Motor Skills with Policy Gradients](http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Neural-Netw-2008-21-682_4867%5b0%5d.pdf) (2008): Proposes a policy gradient method for reinforcement learning of motor skills that improves sample efficiency.
- [Approximately Optimal Approximate Reinforcement Learning](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf) (2002): Proposes an approximately optimal approximate reinforcement learning method that improves sample efficiency.
- [A Natural Policy Gradient](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf) (2002): Proposes a natural policy gradient method for reinforcement learning that improves sample efficiency.
- [Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf) (2009): Provides an overview of reinforcement learning algorithms, including model-based and model-free methods, and their applications.
